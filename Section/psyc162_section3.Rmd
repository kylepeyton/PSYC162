---
title: "Evolution of Cooperation (Section 3)"
author: ""
date: "14 February 2017"
output: 
  beamer_presentation:
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library("hop")
```


# Repeated interactions

- "Folk Theorem": any level of cooperation can be achieved (as NE) in infinitely repeated game provided players are "sufficiently patient".
- Applies to *any* normal form game, not just the Prisoner's Dilemma.
- Lots of equilibria possible even if single-period game only has one! 
- **Problem:** if repeated interactions are *finite*, then people may not cooperate in any period.

# Repeated interactions

Example: Greenhouse gas emissions

- US and China account for 1/2 of world's emissions. 
- Clean air is a public good, pollution is bad for everyone. 
- Why can't we reduce emissions?
- "America is not a planet. And we are not even the largest carbon producer anymore, China is [...] I am not in favor of any policies that make America a harder place for people to live." - Marco Rubio
- But if US/China are playing a repeated game, maybe there is hope!

# Repeated interactions

\vspace{-0.95cm}
```{r}
payoffs <- matrix(c(2,3,0,1),2)
layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), heights=c(1.5,1))
par(mar=c(5.1,5.1,5.1,3.1))
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", tscale = .75,
            asp = 0.7)
gt_folk(payoffs, rational = FALSE, main = "Feasible payoffs",
        mainsize = 1.5)
gt_folk(payoffs, feasible = FALSE, main = "Individually rational payoffs",
        mainsize = 1.5)
gt_folk(payoffs, main = "Supportable payoffs", mainsize = 1.5, fine = 200)
```


# Repeated interactions

```{r, fig.height=4,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7)
```

Always cooperate v. Always defect:

- ALLC: C C C C C C = 0
- ALLD: D D D D D D = 18

# Repeated interactions

```{r, fig.height=4,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7)
```

Tit for Tat v. Always defect:

- TFT: C D D D D D =  5
- ALLD: D D D D D D = 8

# Repeated interactions

```{r, fig.height=4,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7)
```

Tit for Tat v. Tit for Tat:

- TFT: C C C C C C =  12
- TFT: C C C C C C = 12

# Repeated interactions

- Payoff matrix for **repeated game** ($t=6$):

```{r}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT","ALLD", "ALLC"), P1 = "US", P2 = "China",
            tscale = .75, labelsize = 0.80, nash = FALSE, arrow1 = FALSE, 
            arrow2 = FALSE, playersize = 1, asp = 0.7)
```

# Repeated interactions

- US best responses (vertical arrows)

```{r}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT","ALLD", "ALLC"), P1 = "US", P2 = "China",
            tscale = .75, labelsize = 0.80,
            nash = FALSE, arrow1 = FALSE, arrow2 = TRUE, 
            playersize = 1, asp = 0.7)
```
 
# Repeated interactions
- China best responses (horizontal arrows)
 
```{r}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT","ALLD", "ALLC"), P1 = "US", P2 = "China",
            tscale = .75, labelsize = 0.80, asp = 0.7,
            nash = FALSE, arrow1 = TRUE, arrow2 = TRUE, 
            playersize = 1)
```

# Repeated interactions

- TFT not *strict* Nash equilibria. 
 
```{r}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT","ALLD", "ALLC"), P1 = "US", P2 = "China",
            tscale = .75, labelsize = 0.80, nash = TRUE, arrow1 = TRUE, 
            arrow2 = TRUE, asp = 0.7, playersize = 1)
```

# Repeated interactions (evolution)

- Finite example from before $(t=6)$, now w/ generic evolutionary dynamics

```{r, fig.height=4, fig.width=6}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT", "ALLD", "ALLC"), P1 = "", P2 = "",
            tscale = .5, labelsize = 0.80, nash = TRUE, arrow1 = TRUE, 
            arrow2 = TRUE, playersize = 1, asp = 0.7)
```

- Suppose $p$ play TFT, $q$ play ALLD, $1-q-p$ play ALLC

# Repeated interactions (evolution)

```{r, fig.height=4, fig.width=6}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT", "ALLD", "ALLC"), P1 = "", P2 = "",
            tscale = .5, labelsize = 0.80, nash = TRUE, arrow1 = TRUE, 
            arrow2 = TRUE, playersize = 1, asp = 0.7)
```

- $\pi_{TFT}=12p+5q+12(1-q-p)=12-7q$
- $\pi_{ALLD}=8p+6q+18(1-q-p)=18-12q-10p$
- $\pi_{ALLC}=12p+12(1-q-p)=12-12q$


# Repeated interactions (evolution)

```{r, fig.height=4, fig.width=6}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT", "ALLD", "ALLC"), P1 = "", P2 = "",
            tscale = .5, labelsize = 0.80, nash = TRUE, arrow1 = TRUE, 
            arrow2 = TRUE, playersize = 1, asp = 0.7)
```

- Let $p=q=1/3$ 
- $\pi_{TFT}=12-7/3\approx 9.67$
- $\pi_{ALLD}=18-12/3-10/3 \approx 10.67$
- $\pi_{ALLC}=12-12/3 \approx 8$
- ALLD is risk dominant, largest basin of attraction. Also *strict* NE $\Rightarrow$ ESS. 

# Repeated interactions (evolution)

```{r, fig.height=4, fig.width=6}
payoffs <- matrix(c(12,8,12,5,6,0,12,18,12), 3)
gt_bimatrix(payoffs, labels1=c("TFT", "ALLD", "ALLC"), P1 = "", P2 = "",
            tscale = .5, labelsize = 0.80, nash = TRUE, arrow1 = TRUE, 
            arrow2 = TRUE, playersize = 1, asp = 0.7)
```


- **"Neutral drift"** - ALLC and TFT have same payoff in 2x2 game.
- In ALLC v. TFT population, an ALLC mutation cannot be eliminated.
- In mixed population of ALLC and TFT, TFT is not an ESS.
- In 3x3 game, given enough ALLC for ALLD to prey on, TFT dies out!  

# Repeated interactions (backward induction)

- *Continuation probability*: probability of next round occuring $w\in[0,1]$.
- Assume $w$ is fixed and independent across rounds.
- Then probability of still playing after $t$ rounds is $w^t$.
- *Expected* or "mean"number of rounds is: 
\[
\begin{aligned}
1 + w + w^2 + w^3 + \dots  & = 1 + w(1 + w + w^2 + \dots )\\
\text{Math trick: let } x & = 1 + w + w^2 + w^3 + \dots\\
\Rightarrow x & = 1 + wx \\ 
x - wx & = 1 \\
x(1-w) & = 1 \\
x & = \frac{1}{1-w} \\
1 + w + w^2 + w^3 + \dots  & = \frac{1}{1-w}
\end{aligned}
\]

# Repeated interactions (backward induction)

- Payoff of $\pi$ in every **future** round of infinite game is:

\[
\begin{aligned}
\pi(w + w^2 + w^3 + \dots) & = \pi w(1 + w^2+w^3+\dots)\\
& = \pi w\left(\frac{1}{1-w}\right) \\
& = \pi\left(\frac{w}{1-w}\right)
\end{aligned}
\]

- Depending on what strategies are at play, we might get $\pi_0$ in first period and $\pi\left(\frac{w}{1-w}\right)$ in all other periods

# Repeated interactions (backward induction)

Example: both play Grim Trigger

```{r, fig.height=3.5,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7)
```

- *Expected payoffs to cooperation*: $\frac{2}{1-w}$
- *Expected payoffs for defection*: $3 + 1\times\left(\frac{w}{1-w}\right)$
- NE if $\frac{2}{1-w} > 3 + \left(\frac{w}{1-w}\right)$ (if $w>1/2$).
- More generally, we need $w > \frac{T-R}{T-P}$. 

# Repeated interactions (backward induction)

Example: both play Tit-for-Tat (TFT)

```{r, fig.height=3.5,fig.width=5}
payoffs <- matrix(c("R","T","S", "P"),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7, nash = FALSE, arrow1 = FALSE, 
            arrow2 = FALSE)
```

- *Expected payoffs to cooperation*: $\frac{R}{1-w}$
- *Expected payoffs for defection in all periods*: $T+w\frac{P}{1-w}$
- *Expected payoffs for 1 period of defection (oscillation)*: 
\[
\begin{aligned}
T + wS + w^2T + w^3S+\dots & = T + wS + w^2(T + wS) +\dots \\
& = (T+wS)/(1-w^2)
\end{aligned}
\]


# Repeated interactions (backward induction)

- TFT is *strict* NE if:

\[
\begin{aligned}
\frac{R}{1-w} > \max\left\{\frac{(T+wS)}{(1-w^2)},T+w\frac{P}{1-w}\right\}
\end{aligned}
\]

- Some algebra gives:

\[
\begin{aligned}
\frac{R}{1-w} & > \frac{(T+wS)}{(1-w^2)} \\
w & > \frac{T-R}{R-S} \\
\frac{R}{1-w} & > T+w\frac{P}{1-w} \\
 w &  > \frac{T-R}{T-P}
\end{aligned}
\]

- Therefore: $w > \max\left\{ \frac{T-R}{R-S},\frac{T-R}{T-P}\right\}$

# Repeated interactions (backward induction)

- Back to running example of stage game. 

```{r, fig.height=3.5,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "US", P2 = "China", 
            tscale = 0.5, asp = 0.7)
```

- Here, $R=2$, $T=3$, $S=0$, $P=1$. Need: $w > \max\left\{ \frac{T-R}{R-S},\frac{T-R}{T-P}\right\}$
- To guarantee TFT as strict NE here, $w > \max\{\frac{1}{2},\frac{1}{2}\}$

# Repeated interactions (backward induction)

- Suppose $w=3/4$ and both play TFT.
- Payoff to [TFT,TFT] is $\frac{2}{1-0.75}=8$
- Payoff from *one mistake*: $(3+0.75\times0)/(1-0.75^2)\approx 6.89$
- Payoff from switch to ALLD after first period: $3 + \frac{0.75}{(1-0.75)}=6$
- **Mistakes are costly!** Solution? 

 
# Repeated interactions (backward induction)

- Suppose $w=3/4$ and both play TFT.
- Payoff to [TFT,TFT] is $\frac{2}{1-0.75}=8$
- Payoff from *one mistake*: $(3+0.75\times0)/(1-0.75^2)\approx 6.89$
- Payoff from switch to ALLD after first period: $3 + \frac{0.75}{(1-0.75)}=6$
- **Mistakes are costly!** Solution? 
- Correct for mistakes:

\centering \includegraphics[height=4cm, width=5cm]{kisses.jpg}

# Reactive strategies

- Let $q$ denote the probability of cooperating in current period if your opponent cooperated in the previous one.
- Let $p$ denote the probability of cooperating in current period if your opponent defected in previous one.
- There are **infinitely many** types of reactive strategies.
- Try making up your own! 
- Example: TFT has $q=1$ and $p=0$. 
- Example: A "Generous TFT" (GTFT) that has $q=1$ and $p=1/3$. 

# Reactive strategies

Illustration:

```{r, fig.height=3.5,fig.width=5}
payoffs <- matrix(c(2,3,0,1),2)
gt_bimatrix(payoffs, labels1=c("C","D"), P1 = "", P2 = "", 
            tscale = 0.5, asp = 0.7)
```

- GTFT: $C \ C \ C \ \overset{*}{D} \ C \ D \ C \ C \ C \ C \dots$
- GTFT: $C \ C \ C \ C \ D \ C \ \mathbf{C} \ C \ C \ C \dots$
- TFT is better than GTFT against ALLD, but if a cooperative population is reached, GTFT will eventually outcompete TFT. 
- If $p$ is too high, however, then GTFT cannot resist invasion from ALLD.





